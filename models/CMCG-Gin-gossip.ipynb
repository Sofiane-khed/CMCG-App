{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd1bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv,SAGEConv,GCNConv,GINConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import DataLoader, Batch\n",
    "from torch.nn import Sequential, Linear, ReLU, LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98284ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd8c93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples:  3826\n",
      "Test Samples:  1092\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import UPFD\n",
    "train_data1 = UPFD(root=\".\", name=\"gossipcop\", feature=\"profile\", split=\"train\")\n",
    "test_data1 = UPFD(root=\".\", name=\"gossipcop\", feature=\"profile\", split=\"test\")\n",
    "print(\"Train Samples: \", len(test_data1))\n",
    "print(\"Test Samples: \", len(train_data1))\n",
    "test_loader1 = DataLoader(train_data1, batch_size=128, shuffle=False)\n",
    "#print(batch_idx)\n",
    "train_loader1 = DataLoader(test_data1, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dba6e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples:  3826\n",
      "Test Samples:  1092\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import UPFD\n",
    "train_data2= UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"train\")\n",
    "test_data2= UPFD(root=\".\", name=\"gossipcop\", feature=\"bert\", split=\"test\")\n",
    "print(\"Train Samples: \", len(test_data2))\n",
    "print(\"Test Samples: \", len(train_data2))\n",
    "test_loader2 = DataLoader(train_data2, batch_size=128, shuffle=False)\n",
    "train_loader2= DataLoader(test_data2, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4e9626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GINLayer, self).__init__()\n",
    "        self.mlp = Sequential(\n",
    "            Linear(in_channels, out_channels),\n",
    "            ReLU(),\n",
    "            Linear(out_channels, out_channels),\n",
    "        )\n",
    "        self.gin = GINConv(self.mlp)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.gin(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c1f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, hidden_ch, out_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Define the GIN layers\n",
    "        self.conv1 = GINLayer(in_channels, hidden_ch)\n",
    "        self.conv2 = GINLayer(hidden_ch, hidden_ch)\n",
    "        self.conv3 = GINLayer(hidden_ch, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.elu(self.conv1(x, edge_index))\n",
    "        h = F.elu(self.conv2(h, edge_index))\n",
    "        h = F.elu(self.conv3(h, edge_index))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37a059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCoAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_channels1, hidden_channels2, num_heads,dropout_rate=0.01):\n",
    "        super(MultiHeadCoAttentionLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_channels1 = hidden_channels1\n",
    "        self.hidden_channels2 = hidden_channels2\n",
    "\n",
    "        # Define separate attention weights for each head\n",
    "        self.query1 = nn.Linear(hidden_channels1, hidden_channels1 * num_heads, bias=False)\n",
    "        self.key1 = nn.Linear(hidden_channels1, hidden_channels1 * num_heads, bias=False)\n",
    "        self.value1 = nn.Linear(hidden_channels1, hidden_channels1 * num_heads, bias=False)\n",
    "\n",
    "        self.query2 = nn.Linear(hidden_channels2, hidden_channels2 * num_heads, bias=False)\n",
    "        self.key2 = nn.Linear(hidden_channels2, hidden_channels2 * num_heads, bias=False)\n",
    "        self.value2 = nn.Linear(hidden_channels2, hidden_channels2 * num_heads, bias=False)\n",
    "\n",
    "        # Output linear layers\n",
    "        self.out1 = nn.Linear(hidden_channels1 * num_heads, hidden_channels1)\n",
    "        self.out2 = nn.Linear(hidden_channels2 * num_heads, hidden_channels2)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        x1: Tensor of shape [batch_size, hidden_channels1]\n",
    "        x2: Tensor of shape [batch_size, hidden_channels2]\n",
    "        \"\"\"\n",
    "        # Multi-head queries, keys, and values\n",
    "        Q1 = self.query1(x1).view(-1, self.num_heads, self.hidden_channels1)\n",
    "        K2 = self.key2(x2).view(-1, self.num_heads, self.hidden_channels2)\n",
    "        V2 = self.value2(x2).view(-1, self.num_heads, self.hidden_channels2)\n",
    "\n",
    "        Q2 = self.query2(x2).view(-1, self.num_heads, self.hidden_channels2)\n",
    "        K1 = self.key1(x1).view(-1, self.num_heads, self.hidden_channels1)\n",
    "        V1 = self.value1(x1).view(-1, self.num_heads, self.hidden_channels1)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores1 = torch.matmul(Q1, K2.transpose(-2, -1)) / (self.hidden_channels2 ** 0.5)\n",
    "        attn_weights1 = F.softmax(attn_scores1, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights11 = self.dropout(attn_weights1)\n",
    "        attended_x1 = torch.matmul(attn_weights11, V2)\n",
    "\n",
    "        attn_scores2 = torch.matmul(Q2, K1.transpose(-2, -1)) / (self.hidden_channels1 ** 0.5)\n",
    "        attn_weights2 = F.softmax(attn_scores2, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights22 = self.dropout(attn_weights2)\n",
    "        attended_x2 = torch.matmul(attn_weights2, V1)\n",
    "\n",
    "        # Concatenate heads and project back to the original dimension\n",
    "        attended_x1 = attended_x1.view(-1, self.num_heads * self.hidden_channels1)\n",
    "        attended_x2 = attended_x2.view(-1, self.num_heads * self.hidden_channels2)\n",
    "\n",
    "        out_x1 = self.out1(attended_x1) + x1  # Residual connection\n",
    "        out_x2 = self.out2(attended_x2) + x2  # Residual connection\n",
    "\n",
    "        return out_x1, out_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c62e5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedSAGE(nn.Module):\n",
    "    def __init__(self, in_channels1, in_channels2, hidden_channels1, hidden_channels2, out_channels):\n",
    "        super(CombinedSAGE, self).__init__()\n",
    "        self.model1 = GAT(in_channels1, hidden_channels, hidden_channels1, out_channels)\n",
    "        self.model2 = GAT(in_channels2, hidden_channels, hidden_channels2, out_channels)\n",
    "        \n",
    "        \n",
    "        # Multi-Head Co-Attention Layer\n",
    "        self.co_attention = MultiHeadCoAttentionLayer(hidden_channels, hidden_channels, num_heads)\n",
    "\n",
    "        #self.co_attention = CoAttentionLayer(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels + hidden_channels, out_channels)  # Fusion FC layer\n",
    "        #self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x1, x2, edge_index1, edge_index2, batch_idx):\n",
    "        # Pass inputs through individual models\n",
    "        t1 = self.model1(x1, edge_index1)\n",
    "        t2 = self.model2(x2, edge_index2)\n",
    "        #print(t1.shape)\n",
    "        #print(t2.shape)\n",
    "\n",
    "        # Apply co-attention\n",
    "        t1, t2 = self.co_attention(t1, t2)\n",
    "        #print(t1.shape)\n",
    "        #print(t2.shape)\n",
    "\n",
    "        # Fuse features and pool\n",
    "        fused_features = torch.cat((t1, t2), dim=-1)\n",
    "        flatten = global_max_pool(fused_features, batch=batch_idx)\n",
    "        out = self.fc(flatten)\n",
    "        #out1 = self.logsoftmax(out)\n",
    "        return torch.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbc1d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedSAGE(\n",
       "  (model1): GAT(\n",
       "    (conv1): GINLayer(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=10, out_features=26, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=26, out_features=26, bias=True)\n",
       "      )\n",
       "      (gin): GINConv(nn=Sequential(\n",
       "        (0): Linear(in_features=10, out_features=26, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=26, out_features=26, bias=True)\n",
       "      ))\n",
       "    )\n",
       "    (conv2): GINLayer(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=26, out_features=26, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=26, out_features=26, bias=True)\n",
       "      )\n",
       "      (gin): GINConv(nn=Sequential(\n",
       "        (0): Linear(in_features=26, out_features=26, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=26, out_features=26, bias=True)\n",
       "      ))\n",
       "    )\n",
       "    (conv3): GINLayer(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=26, out_features=100, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "      (gin): GINConv(nn=Sequential(\n",
       "        (0): Linear(in_features=26, out_features=100, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "      ))\n",
       "    )\n",
       "  )\n",
       "  (model2): GAT(\n",
       "    (conv1): GINLayer(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=200, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "      )\n",
       "      (gin): GINConv(nn=Sequential(\n",
       "        (0): Linear(in_features=768, out_features=200, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "      ))\n",
       "    )\n",
       "    (conv2): GINLayer(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "      )\n",
       "      (gin): GINConv(nn=Sequential(\n",
       "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "      ))\n",
       "    )\n",
       "    (conv3): GINLayer(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "      (gin): GINConv(nn=Sequential(\n",
       "        (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "      ))\n",
       "    )\n",
       "  )\n",
       "  (co_attention): MultiHeadCoAttentionLayer(\n",
       "    (query1): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (key1): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (value1): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (query2): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (key2): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (value2): Linear(in_features=100, out_features=100, bias=False)\n",
       "    (out1): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (out2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (dropout): Dropout(p=0.01, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels1 = train_data1.num_features # Number of input features for dataset 1\n",
    "hidden_channels1 = 26\n",
    "hidden_channels2 = 200\n",
    "hidden_channels = 100 # Number of hidden features for dataset 1\n",
    "attention_dim=90\n",
    "out_channels = 1 # Number of output features for dataset 1\n",
    "in_channels2 = train_data2.num_features # Number of input features for dataset 2\n",
    "num_heads=1\n",
    "dropout2 = 0.6 # Dropout probability for dataset 2\n",
    "negative_slope2 = 0.2 # Negative slope for LeakyReLU for dataset 2\n",
    "#combined_model = CombinedGAT(model1, model2)\n",
    "CombinedSAGE(in_channels1, in_channels2, hidden_channels1, hidden_channels2, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d58464d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score,roc_auc_score, precision_score, recall_score\n",
    "combined_model=CombinedSAGE(in_channels1, in_channels2, hidden_channels1, hidden_channels2, out_channels)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_fnc = torch.nn.BCELoss()\n",
    "#optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "#num_epochs = 20\n",
    "def metrics(preds, gts):\n",
    "    probs = torch.cat(preds).cpu().numpy()  # raw sigmoid outputs\n",
    "    preds_bin = torch.round(torch.cat(preds)).cpu().numpy()\n",
    "    gts = torch.cat(gts).cpu().numpy()\n",
    "\n",
    "    acc = accuracy_score(gts, preds_bin)\n",
    "    f1 = f1_score(gts, preds_bin, zero_division=0)\n",
    "    prec = precision_score(gts, preds_bin, zero_division=0)\n",
    "    rec = recall_score(gts, preds_bin, zero_division=0)\n",
    "\n",
    "    if len(set(gts)) < 2:\n",
    "        print(\"Cannot compute AUC: only one class present.\")\n",
    "        auc = None\n",
    "    else:\n",
    "        auc = roc_auc_score(gts, probs)  # Correct input: probs, not preds_bin\n",
    "\n",
    "    return acc, f1, auc, prec, rec\n",
    "#print(f'Test Loss: {running_loss2/len(train_loader1.dataset)}')\n",
    "#for epoch in range(num_epochs):\n",
    "def train(epoch):\n",
    "    combined_model.train()\n",
    "    running_loss = 0.0\n",
    "    for data1, data2 in zip(train_loader1, train_loader2):\n",
    "        #combined_batch = Batch.from_data_list([data1, data2])\n",
    "        optimizer.zero_grad()\n",
    "        x1, edge_index1, y1,batch1 = data1.x, data1.edge_index, data1.y,data1.batch\n",
    "        x2, edge_index2, y2,batch2 = data2.x, data2.edge_index, data2.y,data2.batch\n",
    "        out1 = combined_model(x1, x2, edge_index1, edge_index2,batch1)\n",
    "        loss1 = loss_fnc(out1.squeeze(), y1.to(torch.float))\n",
    "        loss1.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss1.item()\n",
    "    return running_loss/len(train_loader1.dataset)\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader1.dataset)}')\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test(epoch):\n",
    "    combined_model.eval()\n",
    "    running_loss2 = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    #with torch.no_grad():\n",
    "    for data1, data2 in zip(test_loader1, test_loader2):\n",
    "        x1, edge_index1, x2, edge_index2, batch11 = data1.x, data1.edge_index, data2.x,data1.edge_index, data1.batch\n",
    "        \n",
    "        xt1, edge_indext1, yt1,batcht1 = data1.x, data1.edge_index, data1.y,data1.batch\n",
    "        xt2, edge_indext2, yt2,batcht2 = data2.x, data2.edge_index, data2.y,data2.batch\n",
    "        outt1= combined_model(xt1, xt2, edge_indext1, edge_indext2, batcht1)\n",
    "        losst1 = loss_fnc(outt1.squeeze(), yt1.to(torch.float))\n",
    "        running_loss2 += losst1.item()\n",
    "        all_preds.append(outt1.squeeze())\n",
    "        #print(all_preds.shape)\n",
    "        all_labels.append(yt1.to(torch.float))\n",
    "        \n",
    "    accuracy, f1,auc,prec,rec = metrics(all_preds, all_labels)\n",
    "    return running_loss2 / len(test_loader1.dataset), accuracy, f1,auc,prec,rec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ddadc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 |  TestAcc: 0.7363 |TestF1: 0.6604 | Test_auc: 0.8490 |Test_prec: 0.8946|Test_rec: 0.5234\n",
      "Epoch: 01 |  TestAcc: 0.8324 |TestF1: 0.8285 | Test_auc: 0.9181 |Test_prec: 0.8308|Test_rec: 0.8262\n",
      "Epoch: 02 |  TestAcc: 0.8590 |TestF1: 0.8539 | Test_auc: 0.9359 |Test_prec: 0.8671|Test_rec: 0.8411\n",
      "Epoch: 03 |  TestAcc: 0.8919 |TestF1: 0.8839 | Test_auc: 0.9569 |Test_prec: 0.9335|Test_rec: 0.8393\n",
      "Epoch: 04 |  TestAcc: 0.8919 |TestF1: 0.8935 | Test_auc: 0.9691 |Test_prec: 0.8639|Test_rec: 0.9252\n",
      "Epoch: 05 |  TestAcc: 0.9295 |TestF1: 0.9252 | Test_auc: 0.9784 |Test_prec: 0.9636|Test_rec: 0.8897\n",
      "Epoch: 06 |  TestAcc: 0.9542 |TestF1: 0.9527 | Test_auc: 0.9901 |Test_prec: 0.9655|Test_rec: 0.9402\n",
      "Epoch: 07 |  TestAcc: 0.9487 |TestF1: 0.9467 | Test_auc: 0.9901 |Test_prec: 0.9650|Test_rec: 0.9290\n",
      "Epoch: 08 |  TestAcc: 0.9570 |TestF1: 0.9571 | Test_auc: 0.9912 |Test_prec: 0.9357|Test_rec: 0.9794\n",
      "Epoch: 09 |  TestAcc: 0.9579 |TestF1: 0.9580 | Test_auc: 0.9902 |Test_prec: 0.9374|Test_rec: 0.9794\n",
      "Epoch: 10 |  TestAcc: 0.9341 |TestF1: 0.9351 | Test_auc: 0.9857 |Test_prec: 0.9026|Test_rec: 0.9701\n",
      "Epoch: 11 |  TestAcc: 0.9661 |TestF1: 0.9659 | Test_auc: 0.9914 |Test_prec: 0.9527|Test_rec: 0.9794\n",
      "Epoch: 12 |  TestAcc: 0.9625 |TestF1: 0.9624 | Test_auc: 0.9914 |Test_prec: 0.9458|Test_rec: 0.9794\n",
      "Epoch: 13 |  TestAcc: 0.9634 |TestF1: 0.9631 | Test_auc: 0.9925 |Test_prec: 0.9508|Test_rec: 0.9757\n",
      "Epoch: 14 |  TestAcc: 0.9652 |TestF1: 0.9652 | Test_auc: 0.9930 |Test_prec: 0.9461|Test_rec: 0.9850\n",
      "Epoch: 15 |  TestAcc: 0.9643 |TestF1: 0.9638 | Test_auc: 0.9937 |Test_prec: 0.9576|Test_rec: 0.9701\n",
      "Epoch: 16 |  TestAcc: 0.9652 |TestF1: 0.9640 | Test_auc: 0.9910 |Test_prec: 0.9770|Test_rec: 0.9514\n",
      "Epoch: 17 |  TestAcc: 0.9679 |TestF1: 0.9676 | Test_auc: 0.9928 |Test_prec: 0.9596|Test_rec: 0.9757\n",
      "Epoch: 18 |  TestAcc: 0.9606 |TestF1: 0.9607 | Test_auc: 0.9911 |Test_prec: 0.9393|Test_rec: 0.9832\n",
      "Epoch: 19 |  TestAcc: 0.9689 |TestF1: 0.9685 | Test_auc: 0.9927 |Test_prec: 0.9613|Test_rec: 0.9757\n",
      "Epoch: 20 |  TestAcc: 0.9698 |TestF1: 0.9694 | Test_auc: 0.9932 |Test_prec: 0.9631|Test_rec: 0.9757\n",
      "Epoch: 21 |  TestAcc: 0.9634 |TestF1: 0.9633 | Test_auc: 0.9933 |Test_prec: 0.9459|Test_rec: 0.9813\n",
      "Epoch: 22 |  TestAcc: 0.9542 |TestF1: 0.9549 | Test_auc: 0.9931 |Test_prec: 0.9232|Test_rec: 0.9888\n",
      "Epoch: 23 |  TestAcc: 0.9579 |TestF1: 0.9578 | Test_auc: 0.9919 |Test_prec: 0.9405|Test_rec: 0.9757\n",
      "Epoch: 24 |  TestAcc: 0.9652 |TestF1: 0.9648 | Test_auc: 0.9931 |Test_prec: 0.9560|Test_rec: 0.9738\n",
      "Epoch: 25 |  TestAcc: 0.9661 |TestF1: 0.9660 | Test_auc: 0.9934 |Test_prec: 0.9511|Test_rec: 0.9813\n",
      "Epoch: 26 |  TestAcc: 0.9643 |TestF1: 0.9635 | Test_auc: 0.9916 |Test_prec: 0.9644|Test_rec: 0.9626\n",
      "Epoch: 27 |  TestAcc: 0.9707 |TestF1: 0.9698 | Test_auc: 0.9942 |Test_prec: 0.9809|Test_rec: 0.9589\n",
      "Epoch: 28 |  TestAcc: 0.9698 |TestF1: 0.9691 | Test_auc: 0.9934 |Test_prec: 0.9700|Test_rec: 0.9682\n",
      "Epoch: 29 |  TestAcc: 0.9579 |TestF1: 0.9567 | Test_auc: 0.9912 |Test_prec: 0.9639|Test_rec: 0.9495\n",
      "Epoch: 30 |  TestAcc: 0.9606 |TestF1: 0.9597 | Test_auc: 0.9915 |Test_prec: 0.9624|Test_rec: 0.9570\n",
      "Epoch: 31 |  TestAcc: 0.9643 |TestF1: 0.9642 | Test_auc: 0.9921 |Test_prec: 0.9477|Test_rec: 0.9813\n",
      "Epoch: 32 |  TestAcc: 0.9652 |TestF1: 0.9651 | Test_auc: 0.9934 |Test_prec: 0.9494|Test_rec: 0.9813\n",
      "Epoch: 33 |  TestAcc: 0.9698 |TestF1: 0.9693 | Test_auc: 0.9930 |Test_prec: 0.9648|Test_rec: 0.9738\n",
      "Epoch: 34 |  TestAcc: 0.9670 |TestF1: 0.9660 | Test_auc: 0.9939 |Test_prec: 0.9752|Test_rec: 0.9570\n",
      "Epoch: 35 |  TestAcc: 0.9597 |TestF1: 0.9596 | Test_auc: 0.9896 |Test_prec: 0.9439|Test_rec: 0.9757\n",
      "Epoch: 36 |  TestAcc: 0.9661 |TestF1: 0.9656 | Test_auc: 0.9929 |Test_prec: 0.9594|Test_rec: 0.9720\n",
      "Epoch: 37 |  TestAcc: 0.9634 |TestF1: 0.9621 | Test_auc: 0.9935 |Test_prec: 0.9750|Test_rec: 0.9495\n",
      "Epoch: 38 |  TestAcc: 0.9560 |TestF1: 0.9557 | Test_auc: 0.9862 |Test_prec: 0.9435|Test_rec: 0.9682\n",
      "Epoch: 39 |  TestAcc: 0.9661 |TestF1: 0.9658 | Test_auc: 0.9924 |Test_prec: 0.9544|Test_rec: 0.9776\n",
      "Epoch: 40 |  TestAcc: 0.9716 |TestF1: 0.9706 | Test_auc: 0.9939 |Test_prec: 0.9846|Test_rec: 0.9570\n",
      "Epoch: 41 |  TestAcc: 0.9679 |TestF1: 0.9667 | Test_auc: 0.9922 |Test_prec: 0.9845|Test_rec: 0.9495\n",
      "Epoch: 42 |  TestAcc: 0.9689 |TestF1: 0.9681 | Test_auc: 0.9938 |Test_prec: 0.9718|Test_rec: 0.9645\n",
      "Epoch: 43 |  TestAcc: 0.9597 |TestF1: 0.9579 | Test_auc: 0.9931 |Test_prec: 0.9823|Test_rec: 0.9346\n",
      "Epoch: 44 |  TestAcc: 0.9643 |TestF1: 0.9627 | Test_auc: 0.9900 |Test_prec: 0.9863|Test_rec: 0.9402\n",
      "Epoch: 45 |  TestAcc: 0.9615 |TestF1: 0.9604 | Test_auc: 0.9925 |Test_prec: 0.9695|Test_rec: 0.9514\n",
      "Epoch: 46 |  TestAcc: 0.9606 |TestF1: 0.9586 | Test_auc: 0.9941 |Test_prec: 0.9881|Test_rec: 0.9308\n",
      "Epoch: 47 |  TestAcc: 0.9597 |TestF1: 0.9579 | Test_auc: 0.9936 |Test_prec: 0.9823|Test_rec: 0.9346\n",
      "Epoch: 48 |  TestAcc: 0.9643 |TestF1: 0.9628 | Test_auc: 0.9948 |Test_prec: 0.9825|Test_rec: 0.9439\n",
      "Epoch: 49 |  TestAcc: 0.9606 |TestF1: 0.9597 | Test_auc: 0.9911 |Test_prec: 0.9624|Test_rec: 0.9570\n",
      "Epoch: 50 |  TestAcc: 0.9478 |TestF1: 0.9443 | Test_auc: 0.9921 |Test_prec: 0.9898|Test_rec: 0.9028\n",
      "Epoch: 51 |  TestAcc: 0.9652 |TestF1: 0.9636 | Test_auc: 0.9952 |Test_prec: 0.9882|Test_rec: 0.9402\n",
      "Epoch: 52 |  TestAcc: 0.9698 |TestF1: 0.9688 | Test_auc: 0.9948 |Test_prec: 0.9790|Test_rec: 0.9589\n",
      "Epoch: 53 |  TestAcc: 0.9588 |TestF1: 0.9571 | Test_auc: 0.9930 |Test_prec: 0.9767|Test_rec: 0.9383\n",
      "Epoch: 54 |  TestAcc: 0.9597 |TestF1: 0.9574 | Test_auc: 0.9953 |Test_prec: 0.9940|Test_rec: 0.9234\n",
      "Epoch: 55 |  TestAcc: 0.9661 |TestF1: 0.9651 | Test_auc: 0.9946 |Test_prec: 0.9752|Test_rec: 0.9551\n",
      "Epoch: 56 |  TestAcc: 0.9643 |TestF1: 0.9632 | Test_auc: 0.9914 |Test_prec: 0.9733|Test_rec: 0.9533\n",
      "Epoch: 57 |  TestAcc: 0.9725 |TestF1: 0.9720 | Test_auc: 0.9928 |Test_prec: 0.9720|Test_rec: 0.9720\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(58):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss, test_acc, test_f1,test_auc,test_prec,test_rec = test(epoch)\n",
    "    print(f'Epoch: {epoch:02d} |  TestAcc: {test_acc:.4f} |'\n",
    "          f'TestF1: {test_f1:.4f} | Test_auc: {test_auc:.4f} |Test_prec: {test_prec:.4f}|Test_rec: {test_rec:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f86a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd3105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
